Q1: architecture of prometheous

Q2: How does the push-gateway works and in what scenario is it useful?
The Pushgateway is used when we have short-lived jobs — like scripts or cron jobs — that start, finish, and exit quickly.
Since Prometheus scrapes metrics at fixed intervals, it may miss these short-lived jobs, because they’re gone before Prometheus checks.
So instead, these jobs push their metrics to the Pushgateway right before they finish.
Then, Prometheus scrapes the Pushgateway every minute (or whatever the interval is), and collects those metrics.


Q3: What is promql give me some example query?
PromQL is the query language used to interact with the Prometheus HTTP server. It allows you to retrieve, filter, and aggregate time-series data.

With PromQL, you can:
🔍 Query metrics based on labels (e.g. method="GET")
📆 Define a time range for your data (like last 5 minutes or 1 hour)
📊 Aggregate data (e.g. sum, avg, rate)
📈 Visualize results as graphs (e.g. in Prometheus UI or Grafana)


Q4: What is exporter? common exporter?
exporter is a small program that collects matrix from a system or application and makes them available in a format that prometheus can understand.
we can have n number of exporter but each exporter will be responsible for specific system,service,node 
like for node matrix = node exporter
like for k8s matrix =  kube-state-matrix >> give info about pod health,pod restart,memory,cpu,how many configmap is created,how many secrets,deployment status
similarly we can have so many exporter for specific matrix,we can also have custom matrix
##point  prometheus cannot directly scrap the matrix from system but it rely on exporter (middleman) which collects matrix and expose them via http endpoint(usually /matrix)
and finally prometheus scrap matrix from here


Q5: What is grafana and data source?
Grafana is an open-source data visualization and dashboard tool.
it allow you to Create dashboards with charts, graphs, and alerts,
Visualize metrics from tools like Prometheus.
It's mainly used in monitoring setups to turn raw metrics into readable visuals.

A Data Source is where Grafana gets the data from.Grafana itself doesn’t store any data— it connects to a Data Source (like Prometheus),fetches the data, and then shows it in
dashboards


Q6: What is service discovery? How does it work?
Service Discovery besically help Prometheus to automatically find the services, pods, or targets they need to monitor — without manually listing each one.
Instead of hardcoding IPs or port or modifying prometheous.yaml, Prometheus uses service discovery to dynamically detect and scrape metrics from new services as they come and go.
so lets say developer instrument the code and expose metrics at /metrics endpoint
now as part of service discovery first we will deploy the app with correct labels

apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app
  labels:
    app: my-app             # 👈 This label identifies the app
spec:
  replicas: 1
  selector:
    matchLabels:
      app: my-app           
  template:
    metadata:
      labels:
        app: my-app         
    spec:
      containers:
        - name: my-app
          image: my-app:latest
          ports:
            - name: http    # 👈 This name will be referenced in the Service and ServiceMonitor
              containerPort: 8080


apiVersion: v1
kind: Service
metadata:
  name: my-app-service
  labels:
    app: my-app              # 👈 This label must match ServiceMonitor's selector.matchLabels
spec:
  selector:
    app: my-app              # 👈 This must match Pod labels from the Deployment
  ports:
    - name: http             # 👈 This port name must match the one in ServiceMonitor 'port'
      port: 8080
      targetPort: http       # 👈 Uses the containerPort name from Deployment

apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: my-app-servicemonitor
  namespace: monitoring       # 👈 Where Prometheus and Operator are deployed
  labels:
    release: prometheus       # ✅ REQUIRED: Matches Prometheus CR's selector
spec:
  selector:
    matchLabels:
      app: my-app             # 👈 Matches the label on the Service
  namespaceSelector:
    matchNames:
      - default               # 👈 Namespace where your app and Service are deployed
  endpoints:
    - port: http              # 👈 Must match the port name in the Service
      path: /metrics          # 👈 Path exposed by your app
      interval: 30s


When using the Prometheus Operator (with ServiceMonitor or PodMonitor), it:
Converts your ServiceMonitor CR into internal Prometheus scrape_configs
Automatically generates the necessary relabel_configs behind the scenes


Q7: what is custom metrics how do you implememnt it?
we have discussed above 
Custom metrics are application-specific metrics that you define and expose in your code.
They help you monitor business logic, app behavior, or domain-specific events — things you can’t get from system-level metrics alone.
❗ Unlike default metrics (like CPU or memory), custom metrics are unique to your app — and you control what they measure

Metric Name	What It Tracks
http_requests_total	Total number of HTTP requests
http_request_duration_seconds	Duration of HTTP requests (histogram)
user_signup_total	How many users signed up
order_processing_failures	Number of failed order processes

Q8: what is lebal & relable in prometheous.yaml
   we dont need it as we using servicemonitor and podmonitor

Q9: what is prometheous.yaml config file.
📄 prometheus.yaml is the main configuration file for Prometheus.
It tells Prometheus:
Where to scrape metrics from
How often to scrape
Which labels to apply
service discovery
How to alert


Q10: How would you create dashboard?
from dashboard UI i will click create-dashboard ____add a new panel__select prometheous as data source__write promql query__& customize visualization
#panels is nothing but a visual componant like graphs,tables and alerts.

Q11: what are the common metrics you have scrap?

EKS:                                                                   Jenkins
kube_pod_container_status_running                                      jenkins_job_last_build_duration_seconds                                                                     
kube_deployment_status_replicas                                        jenkins_job_last_build_result
container_cpu_usage_seconds_total                                      jenkins_executor_count
                                                                       node_cpu_seconds_total
                                                                       node_memory_MemTotal_bytes       totalmemory
                                                                       node_memory_MemAvailable_bytes    available

Q12: how would you implement alert-manager?


Q13: difference between counter, guage and histograms in prometheous?
counter: a metrics that only increases (eg number of https requests)
guage: a metrics that can increase or decrease (eg memory ussage)

Q14: How do you monitor custom application with prometheous?
first instrument code with client librarary (Go, Python, Java)
expose a /metrics endpoint
configure prometheous to scrap the endpoint

Q15: How do you scrap multiple targets in prometheous?
we have to define multiple jobs in promethous.yaml
or you can have service discovery.

Q16: diffrence between serviceMonitor & podMonitor?
🛠️ ServiceMonitor: Used to tell Prometheus to scrape metrics from Kubernetes Services.    eg: node exporter(deamonset) or custom metrics
📦 PodMonitor: Used to scrape metrics directly from Pods, especially useful if there's no Service in front of them.     eg: sidecar container or short-lived pods


Q12: how would you implement alert-manager?

first create alerts.yaml

apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: custom-alert-rules
  namespace: monitoring
  labels:
    release: monitoring # if you installed through then you've to mention the release name of helm, otherwise prometheus will not recognize it
spec:
  groups:
  - name: custom.rules
    rules:
    - alert: HighCpuUsage
      expr: 100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[2m])) * 100) > 50
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "High CPU usage on instance {{ $labels.instance }}"
        description: "CPU usage is above 50% (current value: {{ $value }}%)"
    - alert: PodRestart
      expr: kube_pod_container_status_restarts_total > 2
      for: 0m
      labels:
        severity: critical
      annotations:
        summary: "Pod restart detected in namespace {{ $labels.namespace }}"
        description: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} has restarted {{ $value }} times"


then create ssecret for email

apiVersion: v1
kind: Secret
metadata:
  name: slack-webhook
  namespace: monitoring
type: Opaque
stringData:
  webhook: "https://hooks.slack.com/services/T000/B000/XXXX"  # 👈 Replace with your real webhook URL



finally
create alertmanagerconfig.yaml

apiVersion: monitoring.coreos.com/v1alpha1
kind: AlertmanagerConfig
metadata:
  name: slack-alert-config
  namespace: monitoring
  labels:
    release: monitoring   # Must match alertmanagerConfigSelector in Prometheus
spec:
  route:
    receiver: 'null'       # Default receiver
    repeatInterval: 30m
    routes:
      - matchers:
          - name: alertname
            value: HighCpuUsage
        receiver: 'send-slack'
      - matchers:
          - name: alertname
            value: PodRestart
        receiver: 'send-slack'
        repeatInterval: 5m

  receivers:
    - name: 'send-slack'
      slackConfigs:
        - channel: '#alerts'                        # 👈 your Slack channel
          sendResolved: true
          username: 'prometheus-alert'
          title: '{{ .CommonAnnotations.summary }}'
          text: '{{ .CommonAnnotations.description }}'
          apiURL:
            name: slack-webhook          from secret
            key: webhook
    - name: 'null'


"Yes, I have set up Alertmanager. We created Prometheus alert rules for CPU usage, memory, pod restarts, deployment status, and PVC issues.
Then, we created a Slack webhook and stored it in a Kubernetes Secret. Finally, we configured Alertmanager to use that webhook in the receiver section, 
so alerts are sent directly to Slack.




